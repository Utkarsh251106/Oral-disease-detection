{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83b34c-1a4b-455b-b742-7124d47965d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e67a5-c2b9-4d7f-8c1e-76a56f70ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "img_size = (320, 320)\n",
    "\n",
    "# Function to create a dataset from directories with data augmentation\n",
    "def create_dataset(directory, is_train=False):\n",
    "    if is_train:\n",
    "        # Data augmentation for the training dataset\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            layers.RandomFlip('horizontal'),\n",
    "            layers.RandomRotation(0.2),\n",
    "            layers.RandomZoom(0.2),\n",
    "            layers.RandomContrast(0.2),\n",
    "        ])\n",
    "        dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            directory,\n",
    "            image_size=img_size,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "    else:\n",
    "        dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "            directory,\n",
    "            image_size=img_size,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "# Load datasets with augmentation for training\n",
    "train_dataset = create_dataset(\"/content/drive/MyDrive/dataset/train\", is_train=True)\n",
    "valid_dataset = create_dataset(\"/content/drive/MyDrive/dataset/valid\", is_train=False)\n",
    "test_dataset = create_dataset(\"/content/drive/MyDrive/dataset/test\", is_train=False)\n",
    "\n",
    "# Manually get the class names from the directory (updated part)\n",
    "dataset_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your training dataset\n",
    "class_names = sorted(os.listdir(dataset_dir))  # Sort the subdirectories alphabetically\n",
    "\n",
    "# Print the class names to verify\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# Prefetch datasets for performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "train_labels = np.concatenate([y.numpy() for _, y in train_dataset], axis=0)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', classes=np.unique(train_labels), y=train_labels\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# Build your own custom model (no pre-trained weights)\n",
    "model = models.Sequential([\n",
    "    # Conv Layer 1\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(320, 320, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Conv Layer 2\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Conv Layer 3\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Fully connected layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(len(class_names), activation='softmax')  # Output layer with the number of classes\n",
    "])\n",
    "\n",
    "# Compile the model with learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4, decay_steps=1000, decay_rate=0.9\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=30,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Generate predictions and calculate metrics\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)\n",
    "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"accuracy\": test_acc,\n",
    "    \"classification_report\": report,\n",
    "    \"confusion_matrix\": conf_matrix.tolist()\n",
    "}\n",
    "\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"oral_disease_custom_model.keras\")\n",
    "\n",
    "print(\"Results and model saved successfully.\")\n",
    "\n",
    "# Visualization: Plot F1-Score, Precision, Recall for each class\n",
    "f1_scores = [report[class_name][\"f1-score\"] for class_name in class_names]\n",
    "precision = [report[class_name][\"precision\"] for class_name in class_names]\n",
    "recall = [report[class_name][\"recall\"] for class_name in class_names]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, f1_scores, width, label='F1 Score')\n",
    "rects2 = ax.bar(x, precision, width, label='Precision')\n",
    "rects3 = ax.bar(x + width, recall, width, label='Recall')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Classification Report Metrics')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_plot.png')  # Save plot as image\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix Visualization as Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.savefig('confusion_matrix.png')  # Save confusion matrix plot\n",
    "plt.show()\n",
    "\n",
    "# Create a Zip file to download the model and results\n",
    "with zipfile.ZipFile('model_and_results.zip', 'w') as zipf:\n",
    "    zipf.write('oral_disease_custom_model.keras')\n",
    "    zipf.write('results.json')\n",
    "    zipf.write('metrics_plot.png')\n",
    "    zipf.write('confusion_matrix.png')\n",
    "\n",
    "print(\"All files saved and zipped successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c1b91-b729-4003-925f-d1f9c6e8c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('model_and_results.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4c34f-e802-48cf-a6df-131c157f2b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
